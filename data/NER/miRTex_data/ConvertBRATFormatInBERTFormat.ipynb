{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "import csv\n",
    "from pytorch_pretrained_bert.tokenization import BasicTokenizer\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert BRAT-Format into JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ann = Path('test').glob('*.ann')\n",
    "path_txt = Path('test').glob('*.txt')\n",
    "\n",
    "d = {}\n",
    "\n",
    "for txt,ann in zip(path_txt, path_ann): # iterate over files: \n",
    "    \n",
    "    # because path is object not string\n",
    "    path_t = str(txt) \n",
    "    pmid = path_t[path_t.find('/')+1:path_t.find('.txt')]\n",
    "    #print(pmid)\n",
    "    d[pmid]={}\n",
    "    path_a = str(ann)\n",
    "    file_t = open(path_t, 'r')\n",
    "    lines_t = file_t.readlines()\n",
    "    file_t.close()\n",
    "    file_a = open(path_a, 'r')\n",
    "    entities = csv.reader(file_a, delimiter='\\t')\n",
    "    \n",
    "    list_of_entities = []\n",
    "    for line in entities:\n",
    "        if line[1].split()[0]=='MiRNA':\n",
    "            begin = int(line[1].split()[1])\n",
    "            end = int(line[1].split()[2])\n",
    "            entity = line[2]\n",
    "            list_of_entities.append({'entity':entity, 'begin':begin, 'end':end})\n",
    "    file_a.close()\n",
    "    abstract = lines_t[0]\n",
    "    d[pmid]['abstract'] = abstract\n",
    "    #split abstract into sentences:\n",
    "    list_of_sentences = nltk.tokenize.sent_tokenize(abstract)\n",
    "    d[pmid]['sentences'] = []\n",
    "    begin_sent = 0\n",
    "    for sentence in list_of_sentences:\n",
    "        end_sent = begin_sent + len(sentence)\n",
    "        \n",
    "        # iterate over entities and check wheter the offsets match\n",
    "        entities_per_sentence = []\n",
    "        for ent_d in list_of_entities:\n",
    "            if ent_d['begin']>= begin_sent and ent_d['end']<= end_sent:\n",
    "                entities_per_sentence.append(ent_d)\n",
    "        \n",
    "        if len(entities_per_sentence)!=0:\n",
    "            d[pmid]['sentences'].append({'sentence':sentence, 'begin':begin_sent, 'end':end_sent, \n",
    "                                        'entities':entities_per_sentence})\n",
    "        else:\n",
    "            d[pmid]['sentences'].append({'sentence':sentence, 'begin':begin_sent, 'end':end_sent})\n",
    "        \n",
    "        \n",
    "        begin_sent = end_sent + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('miRTex_test.json', 'w', encoding='utf-8')\n",
    "print(json.dumps(d, indent=3, ensure_ascii=False), file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert BRAT Format into BERT Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(miR)-17-92\n",
      "['For', 'example', ',', 'the', 'miRNA', '(', 'miR', ')-17-92', 'cluster', 'has', 'been', 'characterized', 'as', 'an', 'oncogene', ',', 'while', 'let-7', 'represses', 'Ras', 'and', 'miR-15a', '/', '-16-1', 'represses', 'Bcl-2', ',', 'thereby', 'acting', 'as', 'tumor', 'suppressors', '.']\n",
      "16784027\n",
      "(miR)-124a\n",
      "['Here', ',', 'we', 'demonstrate', 'that', 'human', 'IkappaBzeta', 'is', 'posttranscriptionally', 'regulated', 'by', 'microRNA', '(', 'miR', ')-124a', '.']\n",
      "19502795\n",
      "micro(mi)RNA-146a\n",
      "['We', 'also', 'investigated', 'the', 'changes', 'in', 'micro', '(', 'mi', ')RNA-146a', ',', 'miRNA-155', ',', 'and', 'TNF', 'receptor', '1', '(', 'TNFR1', ')', 'expression', 'after', 'stimulation', '.']\n",
      "22876745\n",
      "hsa-miR-196a2*\n",
      "['Manipulating', 'the', 'hsa-miR-196a2*-TP63', 'axis', 'might', 'provide', 'a', 'potential', 'tumor-suppressive', 'strategy', 'to', 'alleviate', 'the', 'aggressive', 'behavior', 'and', 'poor', 'prognosis', 'of', 'some', 'ERalpha-positive', 'as', 'well', 'as', 'many', 'ERalpha-negative', 'breast', 'cancers', '.']\n",
      "23250869\n",
      "(miRNA)-196a2\n",
      "['BACKGROUND', '/', 'AIMS', ':', 'To', 'accurately', 'evaluate', 'the', 'impact', 'of', 'the', 'C', '/', 'T', 'polymorphism', 'in', 'microRNA', '(', 'miRNA', ')-196a2', 'on', 'the', 'colorectal', 'cancer', '(', 'CRC', ')', 'risk', ',', 'by', 'meta-analysis', '.']\n",
      "24107909\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "path_ann = Path('development').glob('*.ann')\n",
    "path_txt = Path('development').glob('*.txt')\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "c = 0\n",
    "for txt,ann in zip(path_txt, path_ann): # iterate over files: \n",
    "    \n",
    "    # because path is object not string\n",
    "    path_t = str(txt) \n",
    "    pmid = path_t[path_t.find('/')+1:path_t.find('.txt')]\n",
    "    path_a = str(ann)\n",
    "    file_t = open(path_t, 'r')\n",
    "    lines_t = file_t.readlines()\n",
    "    file_t.close()\n",
    "    file_a = open(path_a, 'r')\n",
    "    entities = csv.reader(file_a, delimiter='\\t')\n",
    "    \n",
    "    list_of_entities = []\n",
    "    for line in entities:\n",
    "        if line[1].split()[0]=='MiRNA':\n",
    "            begin = int(line[1].split()[1])\n",
    "            end = int(line[1].split()[2])\n",
    "            entity = line[2]\n",
    "            list_of_entities.append({'entity':entity, 'begin':begin, 'end':end})\n",
    "    file_a.close()\n",
    "\n",
    "    abstract = lines_t[0]\n",
    "    #split abstract into sentences:\n",
    "    list_of_sentences = nltk.tokenize.sent_tokenize(abstract)\n",
    "    #print(list_of_sentences)\n",
    "    begin_sent = 0\n",
    "    \n",
    "    for sentence in list_of_sentences:\n",
    "        splitted_sentence = sentence.replace(',',' ,').replace('-regulated',' -regulated').replace('-LV', ' -LV').replace('Pre-', 'Pre- ').replace('Anti-', 'Anti- ').replace('-mediated', ' -mediated').replace('-HDAC4', ' -HDAC4').replace('mTOR-', 'mTOR- ').replace('Rawq01+', 'Rawq01+ ').replace('-ICAM-1', ' -ICAM-1').replace(';', ' ;').replace('anti-', 'anti- ').replace('/', ' / ').replace('as-', 'as- ').replace('-induced', ' -induced').replace('pre-', ' pre- ').replace('.', ' .').replace('(', ' ( ').replace(')', ' )').replace('-reducing', ' -reducing').replace(':', ' :').replace('-transfected', ' -transfected').split()\n",
    "        temp_labels = ['O' for i in range(len(splitted_sentence))]\n",
    "        #print(splitted_sentence)\n",
    "        #print(temp_labels)\n",
    "        end_sent = begin_sent + len(sentence)\n",
    "        \n",
    "        # iterate over entities and check wheter the offsets match\n",
    "        entities_per_sentence = []\n",
    "        for ent_d in list_of_entities:\n",
    "            if ent_d['begin']>= begin_sent and ent_d['end']<= end_sent:\n",
    "                entities_per_sentence.append(ent_d)\n",
    "        \n",
    "        if len(entities_per_sentence)!=0:\n",
    "            index = 0\n",
    "            for ent in entities_per_sentence:\n",
    "                #print(ent['entity'], index)\n",
    "                #print(splitted_sentence, splitted_sentence[index:])\n",
    "                \n",
    "              #  if ent['entity'] not in splitted_sentence:\n",
    "                if ' ' in ent['entity'] or '/' in ent['entity']:\n",
    "                    e_splitted = ent['entity'].split()\n",
    "                    if e_splitted[0] in splitted_sentence:\n",
    "                        index = splitted_sentence.index(e_splitted[0])\n",
    "                        temp_labels[index]='B'\n",
    "                        for ind in range(1, len(e_splitted)):\n",
    "                            temp_labels[index]=='I'\n",
    "                    \n",
    "                elif ent['entity'] in splitted_sentence[index:]:\n",
    "                    index = splitted_sentence[index:].index(ent['entity'])+index\n",
    "                    temp_labels[index]='B'\n",
    "                elif ent['entity'] in splitted_sentence:\n",
    "                    index = splitted_sentence.index(ent['entity'])\n",
    "                    temp_labels[index]='B'\n",
    "                \n",
    "                else:                      \n",
    "                    print(ent['entity'])\n",
    "                    print(splitted_sentence)\n",
    "                    print(pmid)\n",
    "                    c += 1\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                #print(index)\n",
    "            #print(entities_per_sentence)\n",
    "        \n",
    "        begin_sent = end_sent + 1\n",
    "        sentences.append(splitted_sentence)\n",
    "        labels.append(temp_labels)\n",
    "        #print(splitted_sentence)\n",
    "        #print(temp_labels)\n",
    "    #break\n",
    "print(c)\n",
    "\n",
    "### for the remaining ones, I did it manually ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['The', 'most', 'interesting', 'results', 'came', 'from', 'miR-221', ',', 'strongly', 'up', '-regulated', 'in', 'glioblastoma', 'and', 'from', 'a', 'set', 'of', 'brain-enriched', 'miRNAs', ',', 'miR-128', ',', 'miR-181a', ',', 'miR-181b', ',', 'and', 'miR-181c', ',', 'which', 'are', 'down', '-regulated', 'in', 'glioblastoma', '.']\n"
     ]
    }
   ],
   "source": [
    "len(labels[0])\n",
    "print(labels[5])\n",
    "print(sentences[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences_and_labels(labels, sent):\n",
    "\n",
    "    tokenizer = BasicTokenizer(do_lower_case=False)\n",
    "\n",
    "    tokenized_labels = []\n",
    "    tokenized_sentences = []\n",
    "\n",
    "    for i in range(len(sent)):\n",
    "        tokenized_labels_temp = []\n",
    "        tokenized_sentences_temp = []\n",
    "        #for word, label in zip(sent[i].split(), labels[i]):\n",
    "        for word, label in zip(sent[i], labels[i]):\n",
    "            if len(tokenizer.tokenize(word)) > 1:\n",
    "                temp_words = tokenizer.tokenize(word)\n",
    "                tokenized_sentences_temp.append(temp_words[0])\n",
    "                tokenized_labels_temp.append(label)\n",
    "                for w in range(1, len(temp_words)):\n",
    "                    tokenized_sentences_temp.append(temp_words[w])\n",
    "                    if label == 'B':\n",
    "                        tokenized_labels_temp.append('I')\n",
    "                    else:\n",
    "                        tokenized_labels_temp.append('O')\n",
    "\n",
    "            else:\n",
    "                tokenized_sentences_temp.append(word)\n",
    "                tokenized_labels_temp.append('O')\n",
    "\n",
    "        tokenized_sentences.append(tokenized_sentences_temp)\n",
    "        tokenized_labels.append(tokenized_labels_temp)\n",
    "\n",
    "    return tokenized_sentences, tokenized_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, l = tokenize_sentences_and_labels(labels, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tsv_file(output, sentences, labels):\n",
    "    with open(output, 'w', encoding='utf-8') as f:\n",
    "        for i in range(len(labels)):\n",
    "            for word, label in zip(sentences[i], labels[i]):\n",
    "                f.write(\"%s\\t%s\\n\" % (word, label))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tsv_file('test_miRTex.tsv', s, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
